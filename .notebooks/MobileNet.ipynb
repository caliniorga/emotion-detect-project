{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification using MobileNet\n",
    "In essence, this code creates a model that uses a pre-trained MobileNet network for feature extraction and then adds fully connected layers for classification. The fully connected layer is the part that will be further trained to adapt to the specific data of your classification problem, while the MobileNet layers remain unchanged and are used to extract features from the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input \n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MobileNet model with an input shape of 224x224 pixels and 3 color channels (RGB).\n",
    "# The parameter include_top=False means that the fully connected classification layers are not included.\n",
    "model = MobileNet(input_shape=(224, 224, 3), include_top=False)\n",
    "\n",
    "# To prevent further training of the MobileNet layers, all layers in the model are set as non-trainable.\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a Flatten layer to convert the model's output into a one-dimensional vector.\n",
    "x = Flatten()(model.output)\n",
    "\n",
    "# Add a Dense layer with 7 units and a 'softmax' activation function for final classification.\n",
    "x = Dense(units=7, activation='softmax')(x)\n",
    "\n",
    "# Create a new model that combines the MobileNet architecture with the added layers for classification.\n",
    "model = Model(model.input, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=categorical_crossentropy, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for training and testing\n",
    "This code prepares data for training and testing an image classification model using a pre-trained MobileNet architecture.\n",
    "\n",
    "Data Augmentation for Training: Training data is augmented with zooming, shearing, horizontal flipping, and pixel value rescaling, enhancing its diversity and quality.\n",
    "\n",
    "Loading Training Data: The training data is loaded from a specified directory. Images are resized to 224x224 pixels, and batches of 32 images are used.\n",
    "\n",
    "Data Augmentation for Testing: For testing data, only pixel value rescaling is applied.\n",
    "\n",
    "Loading Testing Data: Testing data is loaded from the designated directory.\n",
    "\n",
    "This approach leverages a pre-trained MobileNet network for feature extraction, with additional fully connected layers for classification. The fully connected layers are trained to adapt to the specific classification task, while the MobileNet layers remain unaltered to extract meaningful features from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_augumentation = ImageDataGenerator(zoom_range=0.2, shear_range=0.2, horizontal_flip=True, rescale=1./225)\n",
    "train_data = train_data_augumentation.flow_from_directory(directory=r\"C:\\Users\\Calin PC\\Documents\\emotion-detect-project\\data\\train\", target_size=(224,224), batch_size=32)\n",
    "train_data.class_indices\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_augumentation = ImageDataGenerator(rescale = 1./225)\n",
    "test_data = test_data_augumentation.flow_from_directory(r\"C:\\Users\\Calin PC\\Documents\\emotion-detect-project\\data\\test\")\n",
    "test_data.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping and Model Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\", min_delta=0.01, patience=5, verbose=1)\n",
    "mc = ModelCheckpoint(filepath=\"best_model.h5\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')\n",
    "call_back = [es,mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Calin PC\\AppData\\Local\\Temp\\ipykernel_12344\\1036444180.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hist = model.fit_generator(train_data,\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Calin PC\\Documents\\emotion-detect-project\\.notebooks\\MobileNet.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Calin%20PC/Documents/emotion-detect-project/.notebooks/MobileNet.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hist \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit_generator(train_data, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Calin%20PC/Documents/emotion-detect-project/.notebooks/MobileNet.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                            steps_per_epoch\u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Calin%20PC/Documents/emotion-detect-project/.notebooks/MobileNet.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                            epochs\u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Calin%20PC/Documents/emotion-detect-project/.notebooks/MobileNet.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                            validation_data\u001b[39m=\u001b[39;49m test_data, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Calin%20PC/Documents/emotion-detect-project/.notebooks/MobileNet.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                            validation_steps\u001b[39m=\u001b[39;49m \u001b[39m8\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Calin%20PC/Documents/emotion-detect-project/.notebooks/MobileNet.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                            callbacks\u001b[39m=\u001b[39;49m[es,mc])\n",
      "File \u001b[1;32mc:\\Users\\Calin PC\\Documents\\emotion-detect-project\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:2889\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2877\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2878\u001b[0m \n\u001b[0;32m   2879\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2880\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[0;32m   2881\u001b[0m \u001b[39m  use this endpoint.\u001b[39;00m\n\u001b[0;32m   2882\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2883\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2884\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2885\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2886\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2887\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2888\u001b[0m )\n\u001b[1;32m-> 2889\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   2890\u001b[0m     generator,\n\u001b[0;32m   2891\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   2892\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   2893\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2894\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   2895\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[0;32m   2896\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   2897\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[0;32m   2898\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   2899\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2900\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2901\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2902\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   2903\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   2904\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Calin PC\\Documents\\emotion-detect-project\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Calin PC\\Documents\\emotion-detect-project\\venv\\Lib\\site-packages\\keras\\src\\preprocessing\\image.py:2526\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2482\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.preprocessing.image.apply_affine_transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_affine_transform\u001b[39m(\n\u001b[0;32m   2484\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2496\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   2497\u001b[0m ):\n\u001b[0;32m   2498\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies an affine transformation specified by the parameters given.\u001b[39;00m\n\u001b[0;32m   2499\u001b[0m \n\u001b[0;32m   2500\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2524\u001b[0m \u001b[39m        ImportError: if SciPy is not available.\u001b[39;00m\n\u001b[0;32m   2525\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2526\u001b[0m     \u001b[39mif\u001b[39;00m scipy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2527\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage transformations require SciPy. Install SciPy.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2529\u001b[0m     \u001b[39m# Input sanity checks:\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     \u001b[39m# 1. x must 2D image with one or more channels (i.e., a 3D tensor)\u001b[39;00m\n\u001b[0;32m   2531\u001b[0m     \u001b[39m# 2. channels must be either first or last dimension\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(train_data, \n",
    "                           steps_per_epoch= 10, \n",
    "                           epochs= 30, \n",
    "                           validation_data= test_data, \n",
    "                           validation_steps= 8, \n",
    "                           callbacks=[es,mc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
